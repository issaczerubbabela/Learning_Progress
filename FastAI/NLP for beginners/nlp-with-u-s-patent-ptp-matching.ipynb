{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":33657,"databundleVersionId":3279164,"sourceType":"competition"}],"dockerImageVersionId":30588,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-29T05:52:01.866138Z","iopub.execute_input":"2023-11-29T05:52:01.866440Z","iopub.status.idle":"2023-11-29T05:52:02.219946Z","shell.execute_reply.started":"2023-11-29T05:52:01.866415Z","shell.execute_reply":"2023-11-29T05:52:02.218977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First check if the environment is kaggle notebook.","metadata":{}},{"cell_type":"code","source":"#!pip download datasets\n#!pip download transformers\n!pip install -q transformers\n!pip install 'transformers[torch]'","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:11:30.909131Z","iopub.execute_input":"2023-11-29T06:11:30.909497Z","iopub.status.idle":"2023-11-29T06:11:54.264785Z","shell.execute_reply.started":"2023-11-29T06:11:30.909466Z","shell.execute_reply":"2023-11-29T06:11:54.263741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:52:02.227915Z","iopub.execute_input":"2023-11-29T05:52:02.228298Z","iopub.status.idle":"2023-11-29T05:52:35.437549Z","shell.execute_reply.started":"2023-11-29T05:52:02.228272Z","shell.execute_reply":"2023-11-29T05:52:35.436401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:09:23.449266Z","iopub.execute_input":"2023-11-29T06:09:23.449623Z","iopub.status.idle":"2023-11-29T06:10:03.701944Z","shell.execute_reply.started":"2023-11-29T06:09:23.449593Z","shell.execute_reply":"2023-11-29T06:10:03.700920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:52:35.441844Z","iopub.execute_input":"2023-11-29T05:52:35.442164Z","iopub.status.idle":"2023-11-29T05:52:35.446966Z","shell.execute_reply.started":"2023-11-29T05:52:35.442137Z","shell.execute_reply":"2023-11-29T05:52:35.445980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"import `Path` to use it to store path to the dataset.\n> `pathlib.Path` : \nit is used to treat datapath as an object and manipulate as such. provides several functions and attributes for use.\n\n","metadata":{}},{"cell_type":"code","source":"from pathlib import Path","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:52:35.448262Z","iopub.execute_input":"2023-11-29T05:52:35.448597Z","iopub.status.idle":"2023-11-29T05:52:35.459323Z","shell.execute_reply.started":"2023-11-29T05:52:35.448566Z","shell.execute_reply":"2023-11-29T05:52:35.458561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"install datasets. go [here](#another_cell) to understand why we are using dataset module.\n<a id='this_cell'></a>","metadata":{}},{"cell_type":"code","source":"if iskaggle:\n    path = Path('../input/us-patent-phrase-to-phrase-matching')\n    ! pip install -q datasets\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:52:35.460833Z","iopub.execute_input":"2023-11-29T05:52:35.461099Z","iopub.status.idle":"2023-11-29T05:53:07.417662Z","shell.execute_reply.started":"2023-11-29T05:52:35.461077Z","shell.execute_reply":"2023-11-29T05:53:07.416626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls {path}","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:07.419149Z","iopub.execute_input":"2023-11-29T05:53:07.419528Z","iopub.status.idle":"2023-11-29T05:53:08.374784Z","shell.execute_reply.started":"2023-11-29T05:53:07.419492Z","shell.execute_reply":"2023-11-29T05:53:08.373812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now we go into the path using the `ls` command used in bash. we see that we have three csv files.  read [this book (python for data analysis by wes kinney)](https://wesmckinney.com/book/) to get to know how to manipulate csv using pandas in python.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(path/'train.csv')\ndf.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:08.376435Z","iopub.execute_input":"2023-11-29T05:53:08.376816Z","iopub.status.idle":"2023-11-29T05:53:08.563842Z","shell.execute_reply.started":"2023-11-29T05:53:08.376780Z","shell.execute_reply":"2023-11-29T05:53:08.562834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:08.565098Z","iopub.execute_input":"2023-11-29T05:53:08.565382Z","iopub.status.idle":"2023-11-29T05:53:08.576693Z","shell.execute_reply.started":"2023-11-29T05:53:08.565357Z","shell.execute_reply":"2023-11-29T05:53:08.575721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:08.580929Z","iopub.execute_input":"2023-11-29T05:53:08.581722Z","iopub.status.idle":"2023-11-29T05:53:08.611547Z","shell.execute_reply.started":"2023-11-29T05:53:08.581680Z","shell.execute_reply":"2023-11-29T05:53:08.610593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### (or)","metadata":{}},{"cell_type":"code","source":"df.input = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:08.612697Z","iopub.execute_input":"2023-11-29T05:53:08.612978Z","iopub.status.idle":"2023-11-29T05:53:08.644795Z","shell.execute_reply.started":"2023-11-29T05:53:08.612954Z","shell.execute_reply":"2023-11-29T05:53:08.643850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"gives the same result. the dataframe column can be accessed using dot access. this column is actually a pandas series.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:08.646163Z","iopub.execute_input":"2023-11-29T05:53:08.646886Z","iopub.status.idle":"2023-11-29T05:53:08.658705Z","shell.execute_reply.started":"2023-11-29T05:53:08.646852Z","shell.execute_reply":"2023-11-29T05:53:08.657661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SideNote :\nfrom bing regarding the datasets module we are importing.","metadata":{}},{"cell_type":"markdown","source":"<a id='another_cell'></a>\nThe `datasets` module in Python is a community-driven open-source library of datasets provided by the HuggingFace team ¹. It is designed to standardize end-user interfaces, versioning, and documentation while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora ¹. **The `datasets` library provides one-line dataloaders for many public datasets, including image datasets, audio datasets, text datasets in 467 languages and dialects, etc. ¹. It also provides efficient data pre-processing for public datasets as well as your own local datasets in CSV, JSON, text, PNG, JPEG, WAV, MP3, Parquet, etc. ¹.** \n\nHere is an example of how to install the `datasets` library using pip:\n\n```python\n! pip install datasets\n```\n\nOnce installed, you can use the `load_dataset()` function to download and pre-process any of the major public datasets provided on the HuggingFace Datasets Hub ¹. For instance, the following code loads the SQuAD dataset:\n\n```python\nsquad_dataset = load_dataset(\"squad\")\n```\n\nI hope this helps!\n\nSource: Conversation with Bing, 11/29/2023\n(1) datasets · PyPI. https://pypi.org/project/datasets/.\n(2) pydataset - a Python Dataset Library - YoungWonks. https://www.youngwonks.com/blog/pydataset-a-python-dataset-library.\n(3) Are there any example data sets for Python? - Stack Overflow. https://stackoverflow.com/questions/16579407/are-there-any-example-data-sets-for-python.\n(4) dataset · PyPI. https://pypi.org/project/dataset/.\n(5) 7. Dataset loading utilities — scikit-learn 1.3.2 documentation. https://scikit-learn.org/stable/datasets.html.\n(6) undefined. https://huggingface.co/docs/datasets/installation.\n(7) undefined. https://huggingface.co/docs/datasets/quickstart.","metadata":{}},{"cell_type":"markdown","source":"we have already dowloaded dataset in [this cell](#this_cell) quietly (by using the `-q` command).","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\nds = Dataset.from_pandas(df)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:08.659913Z","iopub.execute_input":"2023-11-29T05:53:08.660252Z","iopub.status.idle":"2023-11-29T05:53:09.707753Z","shell.execute_reply.started":"2023-11-29T05:53:08.660225Z","shell.execute_reply":"2023-11-29T05:53:09.706665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We store it in a Transformer dataset to tokenize and use to do our nlp. This transformer is from huggingface.","metadata":{}},{"cell_type":"code","source":"ds","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:09.708821Z","iopub.execute_input":"2023-11-29T05:53:09.709256Z","iopub.status.idle":"2023-11-29T05:53:09.715220Z","shell.execute_reply.started":"2023-11-29T05:53:09.709229Z","shell.execute_reply":"2023-11-29T05:53:09.714359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We do two process so that our model can understand our data. That is:\n- [Tokenisation](#tokenisation) : splitting texts into words(or tokens)\n- [Numeralization](#numeralization) : converting tokens to numbers.\\\n\nso now we pick a model to do just this.","metadata":{}},{"cell_type":"code","source":"model_nm = \"/kaggle/working/microsoft/deberta-v3-small\"","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:26:24.651588Z","iopub.execute_input":"2023-11-29T06:26:24.652546Z","iopub.status.idle":"2023-11-29T06:26:24.656955Z","shell.execute_reply.started":"2023-11-29T06:26:24.652506Z","shell.execute_reply":"2023-11-29T06:26:24.655920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/microsoft/deberta-v3-small\", exist_ok=True)\ntokz.save_pretrained = \"/kaggle/working/microsoft/deberta-v3-small\"\nmodel.save_pretrained = \"/kaggle/working/microsoft/deberta-v3-small\"","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:26:00.146353Z","iopub.execute_input":"2023-11-29T06:26:00.146766Z","iopub.status.idle":"2023-11-29T06:26:00.152269Z","shell.execute_reply.started":"2023-11-29T06:26:00.146735Z","shell.execute_reply":"2023-11-29T06:26:00.151306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cache_dir = \"/kaggle/working/deberta-cache\"\n\n# Create the cache directory if it doesn't exist\nos.makedirs(cache_dir, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:07:05.407043Z","iopub.execute_input":"2023-11-29T06:07:05.407409Z","iopub.status.idle":"2023-11-29T06:07:05.411933Z","shell.execute_reply.started":"2023-11-29T06:07:05.407382Z","shell.execute_reply":"2023-11-29T06:07:05.411061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification,AutoTokenizer\nHF_DATASETS_OFFLINE=1 \nTRANSFORMERS_OFFLINE=1\ntokz = AutoTokenizer.from_pretrained(model_nm, local_files_only=True, cache_dir=cache_dir)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, local_files_only=True, cache_dir=cache_dir)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T06:27:16.727078Z","iopub.execute_input":"2023-11-29T06:27:16.727765Z","iopub.status.idle":"2023-11-29T06:27:16.870109Z","shell.execute_reply.started":"2023-11-29T06:27:16.727728Z","shell.execute_reply":"2023-11-29T06:27:16.868818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokz.tokenize(\"hello everyone this is issac working at 2 28 in the morning!\")","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.389118Z","iopub.status.idle":"2023-11-29T05:53:57.389589Z","shell.execute_reply.started":"2023-11-29T05:53:57.389340Z","shell.execute_reply":"2023-11-29T05:53:57.389360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the above code is an example of tokenization of a text.","metadata":{}},{"cell_type":"markdown","source":"now here's a function to tokenize our `inputs` column in the dataframe.","metadata":{}},{"cell_type":"code","source":"def tokenz_func(x): return tokz(x['input'])\ntok_ds = ds.map(tokenz_func, batched = True)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.390755Z","iopub.status.idle":"2023-11-29T05:53:57.391105Z","shell.execute_reply.started":"2023-11-29T05:53:57.390930Z","shell.execute_reply":"2023-11-29T05:53:57.390945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"here mapping runs tokenzing in all rows in parrallel which is faster.","metadata":{}},{"cell_type":"code","source":"tok_ds['input'][0], tok_ds['input_ids'][0]","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.392498Z","iopub.status.idle":"2023-11-29T05:53:57.392954Z","shell.execute_reply.started":"2023-11-29T05:53:57.392727Z","shell.execute_reply":"2023-11-29T05:53:57.392748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"these numbers are ids that each token have in the `vocab` of the tokenizer. we can use the `vocab` function to know the id for a string token. this `vocab` is a `list` btw.","metadata":{}},{"cell_type":"code","source":"tokz.vocab['fia']\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.394276Z","iopub.status.idle":"2023-11-29T05:53:57.394593Z","shell.execute_reply.started":"2023-11-29T05:53:57.394437Z","shell.execute_reply":"2023-11-29T05:53:57.394452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we need to rename the `score` column in the ds as `labels` so that transformers can identify it as the target column.","metadata":{}},{"cell_type":"code","source":"tok_ds = tok_ds.rename_columns({'score':'labels'})","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.396442Z","iopub.status.idle":"2023-11-29T05:53:57.396790Z","shell.execute_reply.started":"2023-11-29T05:53:57.396623Z","shell.execute_reply":"2023-11-29T05:53:57.396639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"a separate **validation set** has to be create so that we an avoid *overfitting*. always have three separate sets of data : \n- training set\n- test set\n- validation set  \n\nlearn what is overfitting and underfitting and how to avoid them from [here](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners?scriptVersionId=95800123&cellId=63). this validation set used to check if our model is underfit, overfit or just right.  \n>This is a set of data that we \"hold out\" from training -- we don't let our model see it at all. If you use the fastai library, it automatically creates a validation set for you if you don't have one, and will always report metrics (measurements of the accuracy of a model) using the validation set.  \nThe validation set is only ever used to see how we're doing. It's never used as inputs to training the model.","metadata":{}},{"cell_type":"code","source":"dds = tok_ds.train_test_split(0.25, seed=42)\ndds","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.398981Z","iopub.status.idle":"2023-11-29T05:53:57.399344Z","shell.execute_reply.started":"2023-11-29T05:53:57.399181Z","shell.execute_reply":"2023-11-29T05:53:57.399197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In `transformers` we use `train_test_split` to split our validation set in which here 25% is used for validation.","metadata":{}},{"cell_type":"markdown","source":"Dr. Rachel Thomas explains [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/) here in this article. which i have yet to read but just procrastinated because my head hurts and it's 3 10 in the morning. ugh.","metadata":{}},{"cell_type":"code","source":"#here's the test set in dataframe\neval_df = pd.read_csv(path/'test.csv')\neval_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.400929Z","iopub.status.idle":"2023-11-29T05:53:57.401294Z","shell.execute_reply.started":"2023-11-29T05:53:57.401128Z","shell.execute_reply":"2023-11-29T05:53:57.401144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a test dataset is created now from the test.csv.\n#this is actually held from training and showing metrics.\n#it is only checked after all the training is done\n#to find the accuracy of the model.\neval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor\neval_ds = Dataset.from_pandas(eval_df).map(tokenz_func, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.402807Z","iopub.status.idle":"2023-11-29T05:53:57.403164Z","shell.execute_reply.started":"2023-11-29T05:53:57.402973Z","shell.execute_reply":"2023-11-29T05:53:57.402988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Read about [The problem with metrics is a big problem for AI](https://www.fast.ai/2019/09/24/metrics/) as well later, just as our sensei jphoward suggested.","metadata":{}},{"cell_type":"markdown","source":"There are lots of metrics we try to maximize or minimize. kaggle competitions usually tell you the metric that they want you to maximize.  \nHere [Pearson Coefficient Constant](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is being calculated between the predicted and actual similarity scores.\" \n>This coefficient is usually abbreviated using the single letter r. It is the most widely used measure of the degree of relationship between two variables.  \nr can vary between -1, which means perfect inverse correlation, and +1, which means perfect positive correlation.\n\n[Here](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners?scriptVersionId=95800123&cellId=91)'s where you can to get the intuition to it.","metadata":{}},{"cell_type":"markdown","source":"#### Snippets to calculate the correlation","metadata":{}},{"cell_type":"code","source":"def corr(x,y): return np.corrcoef(x,y)[0][1]\ndef corr_d(eval_pred): return {'pearson': corr(*eval_pred)}","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.404320Z","iopub.status.idle":"2023-11-29T05:53:57.404655Z","shell.execute_reply.started":"2023-11-29T05:53:57.404486Z","shell.execute_reply":"2023-11-29T05:53:57.404501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Atlast we come to training our model.","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments,Trainer","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.405888Z","iopub.status.idle":"2023-11-29T05:53:57.406239Z","shell.execute_reply.started":"2023-11-29T05:53:57.406077Z","shell.execute_reply":"2023-11-29T05:53:57.406093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"set batch size and number of epochs.  \nset learning rate.  \n> The most important hyperparameter is the learning rate. fastai provides a learning rate finder to help you figure this out, but Transformers doesn't, so you'll just have to use trial and error. The idea is to find the largest value you can, but which doesn't result in training failing.","metadata":{}},{"cell_type":"code","source":"bs = 128\nepochs = 4\nlr = 8e-5","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.407244Z","iopub.status.idle":"2023-11-29T05:53:57.407614Z","shell.execute_reply.started":"2023-11-29T05:53:57.407431Z","shell.execute_reply":"2023-11-29T05:53:57.407449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`TrainingArguments` is used to set arguments by Transformers.","metadata":{}},{"cell_type":"code","source":"args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs, weight_decay=0.01, report_to='none')","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.408965Z","iopub.status.idle":"2023-11-29T05:53:57.409336Z","shell.execute_reply.started":"2023-11-29T05:53:57.409167Z","shell.execute_reply":"2023-11-29T05:53:57.409184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bring model and data together\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\ntrainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                  tokenizer=tokz, compute_metrics=corr_d)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.410498Z","iopub.status.idle":"2023-11-29T05:53:57.410814Z","shell.execute_reply.started":"2023-11-29T05:53:57.410659Z","shell.execute_reply":"2023-11-29T05:53:57.410674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train the model\ntrainer.train();","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.411868Z","iopub.status.idle":"2023-11-29T05:53:57.412205Z","shell.execute_reply.started":"2023-11-29T05:53:57.412045Z","shell.execute_reply":"2023-11-29T05:53:57.412061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now test them on the test set.","metadata":{}},{"cell_type":"code","source":"preds = trainer.predict(eval_ds).predictions.astype(float)\npreds = np.clip(preds, 0, 1) #clip them so that you don't see values above 1 and below 0.\npreds","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.413323Z","iopub.status.idle":"2023-11-29T05:53:57.413629Z","shell.execute_reply.started":"2023-11-29T05:53:57.413473Z","shell.execute_reply":"2023-11-29T05:53:57.413487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now to create a submission file for the competition.","metadata":{}},{"cell_type":"code","source":"import datasets\n\nsubmission = datasets.Dataset.from_dict({\n    'id': eval_ds['id'],\n    'score': preds\n})\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:53:57.415254Z","iopub.status.idle":"2023-11-29T05:53:57.415597Z","shell.execute_reply.started":"2023-11-29T05:53:57.415434Z","shell.execute_reply":"2023-11-29T05:53:57.415449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Unfortunately this is a code competition and internet access is disabled. That means the pip install datasets command we used above won't work if you want to submit to Kaggle. To fix this, you'll need to download the pip installers to Kaggle first, as described here. Once you've done that, disable internet in your notebook, go to the Kaggle leaderboards page, and click the Submission button.\n\n--notes from sensei jphoward","metadata":{}},{"cell_type":"markdown","source":"**go do [disaster tweets](https://www.kaggle.com/datasets/vstepanenko/disaster-tweets/data) as practise.**  \n**grandmaster solution to the competition by our sensei jphoward [here](https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster/)**","metadata":{}}]}